{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from torchmetrics.classification import Recall, Accuracy, AUROC, Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset:\n",
      "   0  1  2  3  4  5  6  7  8  9  ...  150519  150520  150521  150522  150523  \\\n",
      "0  0  0  0  0  0  0  0  0  0  0  ...       0       0       0       0       0   \n",
      "1  0  0  0  0  0  0  0  0  0  0  ...       0       0       0       0       0   \n",
      "2  0  0  0  0  0  0  0  0  0  0  ...       0       0       0       0       0   \n",
      "3  0  0  0  0  0  0  0  0  0  0  ...       1       1       1       1       1   \n",
      "4  0  0  0  0  0  0  0  0  0  0  ...       0       0       0       0       0   \n",
      "\n",
      "   150524  150525  150526  150527         label  \n",
      "0       0       0       0       0  glioma_tumor  \n",
      "1       0       0       0       0  glioma_tumor  \n",
      "2       0       0       0       0  glioma_tumor  \n",
      "3       1       1       1       1  glioma_tumor  \n",
      "4       0       0       0       0  glioma_tumor  \n",
      "\n",
      "[5 rows x 150529 columns]\n",
      "\n",
      "Testing Dataset:\n",
      "   0  1  2  3  4  5  6  7  8  9  ...  150519  150520  150521  150522  150523  \\\n",
      "0  0  0  0  0  0  0  0  0  0  0  ...       0       0       0       0       0   \n",
      "1  0  0  0  0  0  0  0  0  0  0  ...       0       0       0       0       0   \n",
      "2  7  7  7  7  7  7  7  7  7  7  ...       4       4       4       5       5   \n",
      "3  0  0  0  0  0  0  0  0  0  0  ...       0       0       0       0       0   \n",
      "4  0  0  0  0  0  0  0  0  0  0  ...       0       0       0       0       0   \n",
      "\n",
      "   150524  150525  150526  150527         label  \n",
      "0       0       0       0       0  glioma_tumor  \n",
      "1       0       0       0       0  glioma_tumor  \n",
      "2       5       4       4       4  glioma_tumor  \n",
      "3       0       0       0       0  glioma_tumor  \n",
      "4       0       0       0       0  glioma_tumor  \n",
      "\n",
      "[5 rows x 150529 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the training and testing folder paths\n",
    "train_path = [\n",
    "    'archive/training/glioma_tumor',\n",
    "    'archive/training/meningioma_tumor',\n",
    "    'archive/training/no_tumor',\n",
    "    'archive/training/pituitary_tumor'\n",
    "]\n",
    "\n",
    "test_path = [\n",
    "    'archive/testing/glioma_tumor',\n",
    "    'archive/testing/meningioma_tumor',\n",
    "    'archive/testing/no_tumor',\n",
    "    'archive/testing/pituitary_tumor'\n",
    "]\n",
    "\n",
    "# Initialize lists to hold image data and labels for training and testing\n",
    "train_image_data = []\n",
    "train_labels = []\n",
    "\n",
    "test_image_data = []\n",
    "test_labels = []\n",
    "\n",
    "# Define a target image size for resizing (e.g., 224x224 pixels)\n",
    "image_size = (224, 224)\n",
    "\n",
    "# Function to process images and assign labels\n",
    "def process_images(folders, label_set, image_data_list, label_list):\n",
    "    for folder in folders:\n",
    "        # Extract the label from the folder name (e.g., 'glioma_tumor')\n",
    "        label = folder.split('/')[-1]  # Folder name as label (e.g., 'glioma_tumor')\n",
    "        # Get the list of image files in the folder\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):  # Check for image file extensions\n",
    "                # Open the image\n",
    "                image_path = os.path.join(folder, filename)\n",
    "                img = Image.open(image_path)\n",
    "                \n",
    "                # Resize the image to the target size\n",
    "                img = img.resize(image_size)\n",
    "                \n",
    "                # Convert image to an array (now in the form of a 3D array)\n",
    "                img_array = np.array(img)\n",
    "                \n",
    "                # Flatten the image array to turn it into a vector\n",
    "                img_vector = img_array.flatten()\n",
    "                \n",
    "                # Append image data and its label\n",
    "                image_data_list.append(img_vector)\n",
    "                label_list.append(label)\n",
    "\n",
    "# Process the training data\n",
    "process_images(train_path, 'training', train_image_data, train_labels)\n",
    "\n",
    "# Process the testing data\n",
    "process_images(test_path, 'testing', test_image_data, test_labels)\n",
    "\n",
    "# Create dataframes for training and testing datasets\n",
    "train_df = pd.DataFrame(train_image_data)\n",
    "train_df['label'] = train_labels\n",
    "\n",
    "test_df = pd.DataFrame(test_image_data)\n",
    "test_df['label'] = test_labels\n",
    "\n",
    "# Display the first few rows of the training and testing datasets\n",
    "print(\"Training Dataset:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTesting Dataset:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Class Distribution:\n",
      "label\n",
      "3    827\n",
      "0    826\n",
      "1    822\n",
      "2    395\n",
      "Name: count, dtype: int64\n",
      "Train Data \n",
      "Class Percentage:\n",
      "label\n",
      "3    28.815331\n",
      "0    28.780488\n",
      "1    28.641115\n",
      "2    13.763066\n",
      "Name: proportion, dtype: float64\n",
      "Test Data Class Distribution:\n",
      "label\n",
      "1    115\n",
      "2    105\n",
      "0    100\n",
      "3     74\n",
      "Name: count, dtype: int64\n",
      "Test Data \n",
      "Class Percentage:\n",
      "label\n",
      "1    29.187817\n",
      "2    26.649746\n",
      "0    25.380711\n",
      "3    18.781726\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "target_column = 'label'\n",
    "\n",
    "# Check class distribution\n",
    "class_distribution = train_df[target_column].value_counts()\n",
    "\n",
    "# Print the class distribution\n",
    "print(\"Train Data Class Distribution:\")\n",
    "print(class_distribution)\n",
    "\n",
    "# Calculate the percentage of each class\n",
    "class_percentage = train_df[target_column].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the class percentage\n",
    "print(\"Train Data \\nClass Percentage:\")\n",
    "print(class_percentage)\n",
    "\n",
    "# Check class distribution\n",
    "class_distribution = test_df[target_column].value_counts()\n",
    "\n",
    "# Print the class distribution\n",
    "print(\"Test Data Class Distribution:\")\n",
    "print(class_distribution)\n",
    "\n",
    "# Calculate the percentage of each class\n",
    "class_percentage = test_df[target_column].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the class percentage\n",
    "print(\"Test Data \\nClass Percentage:\")\n",
    "print(class_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 LoRA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_LoRA(nn.Module):\n",
    "    def __init__(self, num_classes=2, rank=8):\n",
    "        super(VGG16_LoRA, self).__init__()\n",
    "\n",
    "        # Define the VGG16 convolutional layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "        )\n",
    "\n",
    "        # Fully connected layers (adapted with LoRA)\n",
    "        self.fc1 = nn.Linear(512 * 7 * 7, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "\n",
    "        # Apply LoRA (low-rank adaptation) to fully connected layers\n",
    "        self.lora_fc1 = self.low_rank_adaptation(self.fc1, rank)\n",
    "        self.lora_fc2 = self.low_rank_adaptation(self.fc2, rank)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the convolutional layers\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for fully connected layers\n",
    "\n",
    "        # Forward pass through the fully connected layers with LoRA applied\n",
    "        x = self.lora_fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lora_fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def low_rank_adaptation(self, layer, rank):\n",
    "        \"\"\"\n",
    "        Apply LoRA to a fully connected layer.\n",
    "        Decompose the weight matrix into two low-rank matrices.\n",
    "        \"\"\"\n",
    "        in_features = layer.in_features\n",
    "        out_features = layer.out_features\n",
    "\n",
    "        # Decompose the original weight matrix into low-rank matrices\n",
    "        U = nn.Parameter(torch.randn(in_features, rank) * 0.01)  # Small initialization\n",
    "        V = nn.Parameter(torch.randn(rank, out_features) * 0.01)  # Small initialization\n",
    "        \n",
    "        # LoRA layer does not directly modify the weights; instead, it modifies the forward pass\n",
    "        # to use low-rank approximations during the forward pass.\n",
    "        self.register_parameter(f\"lora_U_{id(layer)}\", U)\n",
    "        self.register_parameter(f\"lora_V_{id(layer)}\", V)\n",
    "\n",
    "        return layer\n",
    "\n",
    "    def forward_lora(self, x, layer_name):\n",
    "        \"\"\"\n",
    "        Apply LoRA-modified forward pass.\n",
    "        \"\"\"\n",
    "        U = getattr(self, f\"lora_U_{layer_name}\")\n",
    "        V = getattr(self, f\"lora_V_{layer_name}\")\n",
    "        \n",
    "        # Perform the matrix multiplication for LoRA\n",
    "        return F.linear(x, U @ V.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_df['label'] = label_encoder.fit_transform(train_df['label'])\n",
    "test_df['label'] = label_encoder.transform(test_df['label'])\n",
    "\n",
    "# Split features and labels\n",
    "X_train = train_df.iloc[:, :-1].values  # All columns except 'label'\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "X_test = test_df.iloc[:, :-1].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Normalize the data to [0, 1] range (optional, depends on the model)\n",
    "X_train_tensor /= 255.0\n",
    "X_test_tensor /= 255.0\n",
    "\n",
    "# Reshape the data from [batch_size, height * width * channels] to [batch_size, channels, height, width]\n",
    "X_train_tensor = X_train_tensor.view(-1, 3, 224, 224)\n",
    "X_test_tensor = X_test_tensor.view(-1, 3, 224, 224)\n",
    "\n",
    "# Create a custom Dataset\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = VGG16_LoRA(num_classes=4, rank=8).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.3765\n",
      "Epoch [2/10], Loss: 1.3538\n",
      "Epoch [3/10], Loss: 1.3506\n",
      "Epoch [4/10], Loss: 1.3495\n",
      "Epoch [5/10], Loss: 1.3476\n",
      "Epoch [6/10], Loss: 1.3512\n",
      "Epoch [7/10], Loss: 1.3503\n",
      "Epoch [8/10], Loss: 1.3503\n",
      "Epoch [9/10], Loss: 1.3493\n",
      "Epoch [10/10], Loss: 1.3498\n"
     ]
    }
   ],
   "source": [
    "# Training loop runtime 186 minutes\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Call the training function\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.25\n",
      "Recall: 0.25\n",
      "Precision: 0.25\n",
      "AUROC: 0.49\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function with torchmetrics\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Initialize metrics for multi-class classification\n",
    "    accuracy = Accuracy(num_classes=4, task=\"multiclass\").to(device)\n",
    "    recall = Recall(num_classes=4, task=\"multiclass\").to(device)\n",
    "    precision = Precision(num_classes=4, task=\"multiclass\").to(device)\n",
    "    auroc = AUROC(num_classes=4, task=\"multiclass\").to(device)\n",
    "\n",
    "    # Track predictions and true labels\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get predictions from logits (outputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Update the metrics\n",
    "            accuracy.update(predicted, labels)\n",
    "            recall.update(predicted, labels)\n",
    "            precision.update(predicted, labels)\n",
    "            auroc.update(outputs, labels)  # Use raw outputs for AUROC\n",
    "\n",
    "            # Collect all labels and predictions for later metrics computation\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_preds.append(predicted.cpu())\n",
    "\n",
    "    # Compute all metrics\n",
    "    accuracy_result = accuracy.compute()\n",
    "    recall_result = recall.compute()\n",
    "    precision_result = precision.compute()\n",
    "    auroc_result = auroc.compute()\n",
    "\n",
    "    # Reset metrics after computation\n",
    "    accuracy.reset()\n",
    "    recall.reset()\n",
    "    precision.reset()\n",
    "    auroc.reset()\n",
    "\n",
    "    print(f\"Accuracy: {accuracy_result:.2f}\")\n",
    "    print(f\"Recall: {recall_result:.2f}\")\n",
    "    print(f\"Precision: {precision_result:.2f}\")\n",
    "    print(f\"AUROC: {auroc_result:.2f}\")\n",
    "\n",
    "    # Optionally, return all predictions if needed for further analysis\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Evaluate the model\n",
    "all_labels, all_preds = evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: {'learning_rate': 0.001, 'batch_size': 32}\n",
      "Epoch [1/3], Loss: 1.4048\n",
      "Epoch [2/3], Loss: 1.3523\n",
      "Epoch [3/3], Loss: 1.3497\n",
      "Accuracy: 29.19%, Precision: 0.2919, Recall: 0.2919\n",
      "Metrics - Accuracy: 29.19%, Precision: 0.2919, Recall: 0.2919 for {'learning_rate': 0.001, 'batch_size': 32}\n",
      "Testing parameters: {'learning_rate': 0.001, 'batch_size': 64}\n",
      "Epoch [1/3], Loss: 1.5053\n",
      "Epoch [2/3], Loss: 1.3577\n",
      "Epoch [3/3], Loss: 1.3549\n",
      "Accuracy: 29.19%, Precision: 0.2919, Recall: 0.2919\n",
      "Metrics - Accuracy: 29.19%, Precision: 0.2919, Recall: 0.2919 for {'learning_rate': 0.001, 'batch_size': 64}\n",
      "Testing parameters: {'learning_rate': 0.005, 'batch_size': 32}\n",
      "Epoch [1/3], Loss: 68689.3991\n",
      "Epoch [2/3], Loss: 113.1936\n",
      "Epoch [3/3], Loss: 157.0089\n",
      "Accuracy: 29.19%, Precision: 0.2919, Recall: 0.2919\n",
      "Metrics - Accuracy: 29.19%, Precision: 0.2919, Recall: 0.2919 for {'learning_rate': 0.005, 'batch_size': 32}\n",
      "Testing parameters: {'learning_rate': 0.005, 'batch_size': 64}\n",
      "Epoch [1/3], Loss: 136287.3804\n",
      "Epoch [2/3], Loss: 1.3617\n",
      "Epoch [3/3], Loss: 1.3643\n",
      "Accuracy: 25.38%, Precision: 0.2538, Recall: 0.2538\n",
      "Metrics - Accuracy: 25.38%, Precision: 0.2538, Recall: 0.2538 for {'learning_rate': 0.005, 'batch_size': 64}\n",
      "Testing parameters: {'learning_rate': 0.01, 'batch_size': 32}\n",
      "Epoch [1/3], Loss: 494018392.6025\n",
      "Epoch [2/3], Loss: 1.4507\n",
      "Epoch [3/3], Loss: 1.3637\n",
      "Accuracy: 25.38%, Precision: 0.2538, Recall: 0.2538\n",
      "Metrics - Accuracy: 25.38%, Precision: 0.2538, Recall: 0.2538 for {'learning_rate': 0.01, 'batch_size': 32}\n",
      "Testing parameters: {'learning_rate': 0.01, 'batch_size': 64}\n",
      "Epoch [1/3], Loss: 637203317.0052\n",
      "Epoch [2/3], Loss: 393364.8050\n",
      "Epoch [3/3], Loss: 100684.0010\n",
      "Accuracy: 26.65%, Precision: 0.2665, Recall: 0.2665\n",
      "Metrics - Accuracy: 26.65%, Precision: 0.2665, Recall: 0.2665 for {'learning_rate': 0.01, 'batch_size': 64}\n",
      "Best Hyperparameters: {'learning_rate': 0.001, 'batch_size': 32}, Best Accuracy: 29.19%\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter values\n",
    "learning_rates = [0.001, 0.005, 0.01]\n",
    "batch_sizes = [32, 64]\n",
    "# optimizers = [\"adam\", \"adamw\", \"sgd\"]\n",
    "# epoch_sizes = [2, 3]\n",
    "\n",
    "# Function to train and evaluate the model with given hyperparameters\n",
    "def train_and_evaluate(params, model, train_dataset, test_loader):\n",
    "    batch_size = params['batch_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "    # optimizer_choice = params['optimizer']\n",
    "    num_epochs = 3 # params['epochs']\n",
    "\n",
    "    # Update DataLoader with the new batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # if optimizer_choice == \"adam\":\n",
    "    #     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # elif optimizer_choice == \"adamw\":\n",
    "    #     optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    # elif optimizer_choice == \"sgd\":\n",
    "    #     optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "    # Metrics\n",
    "    precision_metric = Precision(task=\"multiclass\", num_classes=4).to(device)\n",
    "    recall_metric = Recall(task=\"multiclass\", num_classes=4).to(device)\n",
    "    # auroc_metric = AUROC(task=\"multiclass\", num_classes=4).to(device)\n",
    "    \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_predictions.append(predicted)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    # Concatenate all predictions and labels for metrics\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_metric(all_predictions, all_labels).item()\n",
    "    recall = recall_metric(all_predictions, all_labels).item()\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.2f}%, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "# Run manual grid search\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        # for optimizer_choice in optimizers:\n",
    "            # for num_epochs in epoch_sizes:\n",
    "                param_dict = {\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"batch_size\": batch_size,\n",
    "                    # \"optimizer\": optimizer_choice,\n",
    "                    # \"epochs\": num_epochs\n",
    "                }\n",
    "                print(f\"Testing parameters: {param_dict}\")\n",
    "                \n",
    "                # Initialize a fresh model for each combination\n",
    "                model = VGG16_LoRA(num_classes=4, rank=8).to(device)\n",
    "                \n",
    "                # Evaluate current parameter combination\n",
    "                accuracy, precision, recall = train_and_evaluate(param_dict, model, train_dataset, test_loader)\n",
    "                print(f\"Metrics - Accuracy: {accuracy:.2f}%, Precision: {precision:.4f}, Recall: {recall:.4f} for {param_dict}\")\n",
    "                \n",
    "                # Track the best configuration based on accuracy\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_params = param_dict\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_params}, Best Accuracy: {best_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding best optimizer on 0.001 learning rate, batch_size 16, and epochs 2, 3, runtime 1318 minutes\n",
    "# Testing parameters: {'learning_rate': 0.001, 'batch_size': 16, 'optimizer': 'adam', 'epochs': 2}\n",
    "# Epoch [1/2], Loss: 1.3769\n",
    "# Epoch [2/2], Loss: 1.3528\n",
    "# Accuracy: 18.78%\n",
    "# Metrics - Accuracy: 18.78% for {'learning_rate': 0.001, 'batch_size': 16, 'optimizer': 'adam', 'epochs': 2}\n",
    "\n",
    "# Testing parameters: {'learning_rate': 0.001, 'batch_size': 16, 'optimizer': 'adam', 'epochs': 3}\n",
    "# Epoch [1/3], Loss: 1.3756\n",
    "# Epoch [2/3], Loss: 1.3512\n",
    "# Epoch [3/3], Loss: 1.3497\n",
    "# Accuracy: 29.19%\n",
    "# Metrics - Accuracy: 29.19% for {'learning_rate': 0.001, 'batch_size': 16, 'optimizer': 'adam', 'epochs': 3}\n",
    "\n",
    "# Testing parameters: {'learning_rate': 0.001, 'batch_size': 16, 'optimizer': 'adamw', 'epochs': 2}\n",
    "# Epoch [1/2], Loss: 1.4084\n",
    "# Epoch [2/2], Loss: 1.3523\n",
    "# Accuracy: 25.38%\n",
    "# Metrics - Accuracy: 25.38% for {'learning_rate': 0.001, 'batch_size': 16, 'optimizer': 'adamw', 'epochs': 2}\n",
    "\n",
    "# Testing parameters: {'learning_rate': 0.001, 'batch_size': 16, 'optimizer': 'adamw', 'epochs': 3}\n",
    "# Epoch [1/3], Loss: 1.3808\n",
    "# Epoch [2/3], Loss: 1.3912\n",
    "# Epoch [3/3], Loss: 1.3550\n",
    "# Accuracy: 29.19%\n",
    "# Metrics - Accuracy: 29.19% for {'learning_rate': 0.001, 'batch_size': 16, 'optimizer': 'adamw', 'epochs': 3}\n",
    "\n",
    "# Testing parameters: {'learning_rate': 0.001, 'batch_size': 16, 'optimizer': 'sgd', 'epochs': 2}\n",
    "# Epoch [1/2], Loss: 1.3741\n",
    "# Epoch [2/2], Loss: 1.3575\n",
    "# Accuracy: 25.38%\n",
    "# Metrics - Accuracy: 25.38% for {'learning_rate': 0.001, 'batch_size': 16, 'optimizer': 'sgd', 'epochs': 2}\n",
    "\n",
    "# Testing parameters: {'learning_rate': 0.001, 'batch_size': 16, 'optimizer': 'sgd', 'epochs': 3}\n",
    "# Epoch [1/3], Loss: 1.3728\n",
    "# Epoch [2/3], Loss: 1.3566\n",
    "# Epoch [3/3], Loss: 1.3515\n",
    "# Accuracy: 25.38%\n",
    "# Metrics - Accuracy: 25.38% for {'learning_rate': 0.001, 'batch_size': 16, 'optimizer': 'sgd', 'epochs': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding best learning rate and batch_size, with 3 epochs and adam as optimizer, runtime 315 minutes\n",
    "# Testing parameters: {'learning_rate': 0.001, 'batch_size': 32}\n",
    "# Epoch [1/3], Loss: 1.4048\n",
    "# Epoch [2/3], Loss: 1.3523\n",
    "# Epoch [3/3], Loss: 1.3497\n",
    "# Accuracy: 29.19%, Precision: 0.2919, Recall: 0.2919\n",
    "# Metrics - Accuracy: 29.19%, Precision: 0.2919, Recall: 0.2919 for {'learning_rate': 0.001, 'batch_size': 32}\n",
    "# Testing parameters: {'learning_rate': 0.001, 'batch_size': 64}\n",
    "# Epoch [1/3], Loss: 1.5053\n",
    "# Epoch [2/3], Loss: 1.3577\n",
    "# Epoch [3/3], Loss: 1.3549\n",
    "# Accuracy: 29.19%, Precision: 0.2919, Recall: 0.2919\n",
    "# Metrics - Accuracy: 29.19%, Precision: 0.2919, Recall: 0.2919 for {'learning_rate': 0.001, 'batch_size': 64}\n",
    "# Testing parameters: {'learning_rate': 0.005, 'batch_size': 32}\n",
    "# Epoch [1/3], Loss: 68689.3991\n",
    "# Epoch [2/3], Loss: 113.1936\n",
    "# Epoch [3/3], Loss: 157.0089\n",
    "# Accuracy: 29.19%, Precision: 0.2919, Recall: 0.2919\n",
    "# Metrics - Accuracy: 29.19%, Precision: 0.2919, Recall: 0.2919 for {'learning_rate': 0.005, 'batch_size': 32}\n",
    "# Testing parameters: {'learning_rate': 0.005, 'batch_size': 64}\n",
    "# Epoch [1/3], Loss: 136287.3804\n",
    "# Epoch [2/3], Loss: 1.3617\n",
    "# Epoch [3/3], Loss: 1.3643\n",
    "# Accuracy: 25.38%, Precision: 0.2538, Recall: 0.2538\n",
    "# Metrics - Accuracy: 25.38%, Precision: 0.2538, Recall: 0.2538 for {'learning_rate': 0.005, 'batch_size': 64}\n",
    "# Testing parameters: {'learning_rate': 0.01, 'batch_size': 32}\n",
    "# Epoch [1/3], Loss: 494018392.6025\n",
    "# Epoch [2/3], Loss: 1.4507\n",
    "# Epoch [3/3], Loss: 1.3637\n",
    "# Accuracy: 25.38%, Precision: 0.2538, Recall: 0.2538\n",
    "# Metrics - Accuracy: 25.38%, Precision: 0.2538, Recall: 0.2538 for {'learning_rate': 0.01, 'batch_size': 32}\n",
    "# Testing parameters: {'learning_rate': 0.01, 'batch_size': 64}\n",
    "# Epoch [1/3], Loss: 637203317.0052\n",
    "# Epoch [2/3], Loss: 393364.8050\n",
    "# Epoch [3/3], Loss: 100684.0010\n",
    "# Accuracy: 26.65%, Precision: 0.2665, Recall: 0.2665\n",
    "# Metrics - Accuracy: 26.65%, Precision: 0.2665, Recall: 0.2665 for {'learning_rate': 0.01, 'batch_size': 64}\n",
    "# Best Hyperparameters: {'learning_rate': 0.001, 'batch_size': 32}, Best Accuracy: 29.19%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
